{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from torchsig.datasets.wideband_sig53 import WidebandSig53\n",
    "from torchmetrics.detection import MeanAveragePrecision\n",
    "from torch.utils.data import DataLoader\n",
    "from torchsig.transforms.target_transforms import DescToMaskClass\n",
    "from torchsig.transforms.transforms import Spectrogram, Normalize, Compose, Identity\n",
    "from tqdm import tqdm\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import pytorch_lightning as pl\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import time\n",
    "import os\n",
    "from copy import deepcopy\n",
    "from typing import Any, Callable, Iterable, List, Optional, Tuple, Union\n",
    "import numpy as np\n",
    "import pywt\n",
    "import torch\n",
    "from matplotlib import patches\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.figure import Figure\n",
    "from scipy import ndimage\n",
    "from scipy import signal as sp\n",
    "from torch.utils.data import dataloader\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchsig.utils.visualize import MaskClassVisualizer, mask_class_to_outline, complex_spectrogram_to_magnitude\n",
    "from torchsig.transforms.target_transforms import DescToMaskClass\n",
    "from torchsig.datasets.wideband import WidebandModulationsDataset\n",
    "from torchsig.transforms.transforms import Spectrogram, Normalize, Compose\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "modulation_list = [\n",
    "    \"ook\",\"bpsk\",\"4pam\",\"4ask\",\"qpsk\",\"8pam\",\"8ask\",\"8psk\",\"16qam\",\"16pam\",\n",
    "    \"16ask\",\"16psk\",\"32qam\",\"32qam_cross\",\"32pam\",\"32ask\",\"32psk\",\"64qam\",\"64pam\",\"64ask\",\n",
    "    \"64psk\",\"128qam_cross\",\"256qam\",\"512qam_cross\",\"1024qam\",\"2fsk\",\"2gfsk\",\"2msk\",\"2gmsk\",\"4fsk\",\n",
    "    \"4gfsk\",\"4msk\",\"4gmsk\",\"8fsk\",\"8gfsk\",\"8msk\",\"8gmsk\",\"16fsk\",\"16gfsk\",\"16msk\",\"16gmsk\",\n",
    "    \"ofdm-64\",\"ofdm-72\",\"ofdm-128\",\"ofdm-180\",\"ofdm-256\",\"ofdm-300\",\"ofdm-512\",\"ofdm-600\",\n",
    "    \"ofdm-900\",\"ofdm-1024\",\"ofdm-1200\",\"ofdm-2048\",\n",
    "]    \n",
    "\n",
    "fft_size = 512\n",
    "num_classes = len(modulation_list)\n",
    "num_iq_samples = fft_size * fft_size\n",
    "num_samples = 150\n",
    "\n",
    "data_transform = Compose([\n",
    "    Spectrogram(nperseg=fft_size, noverlap=0, nfft=fft_size, mode='complex'),\n",
    "    Normalize(norm=np.inf, flatten=True),\n",
    "])\n",
    "\n",
    "target_transform = Compose([\n",
    "    DescToMaskClass(num_classes=num_classes, width=fft_size, height=fft_size),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "wideband_modulations_dataset = WidebandModulationsDataset(\n",
    "    modulation_list=modulation_list,\n",
    "    level=1,\n",
    "    num_iq_samples=num_iq_samples,\n",
    "    num_samples=num_samples,\n",
    "    transform=data_transform,\n",
    "    target_transform=target_transform,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "# Specify the split ratio (e.g., 0.8 for 80% train and 20% test)\n",
    "train_ratio = 0.8\n",
    "test_ratio = 1 - train_ratio\n",
    "\n",
    "# Calculate the number of samples for each split\n",
    "num_samples = len(wideband_modulations_dataset)\n",
    "train_size = int(train_ratio * num_samples)\n",
    "test_size = num_samples - train_size\n",
    "\n",
    "# Perform the random split\n",
    "train_dataset, test_dataset = random_split(wideband_modulations_dataset, [train_size, test_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def complex_spectrogram_to_magnitude(tensor: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Visualizer data transform: Transform two channel spectrogram data for\n",
    "    spectrogram magnitude visualization (mode = 'complex')\n",
    "\n",
    "    \"\"\"\n",
    "    batch_size = tensor.shape[0]\n",
    "    new_tensor = np.zeros((batch_size, tensor.shape[2], tensor.shape[3]), dtype=np.float64)\n",
    "    for idx in range(tensor.shape[0]):\n",
    "        new_tensor[idx] = 20 * np.log10(tensor[idx, 0] ** 2 + tensor[idx, 1] ** 2)\n",
    "    return new_tensor\n",
    "\n",
    "def mask_class_to_outline(tensor: np.ndarray) -> Tuple[List[List[int]], List[Any]]:\n",
    "    \"\"\"Target Transform: Transforms masks for each burst to individual outlines\n",
    "    for the MaskClassVisualizer. Overlapping mask outlines are still shown as\n",
    "    overlapping. Each bursts' class index is also returned.\n",
    "\n",
    "    \"\"\"\n",
    "    batch_size = tensor.shape[0]\n",
    "    labels = []\n",
    "    class_idx = []\n",
    "    struct = ndimage.generate_binary_structure(2, 2)\n",
    "    for idx in range(batch_size):\n",
    "        label = tensor[idx].numpy()\n",
    "        class_idx_curr = []\n",
    "        for individual_burst_idx in range(label.shape[0]):\n",
    "            if np.count_nonzero(label[individual_burst_idx]) > 0:\n",
    "                class_idx_curr.append(individual_burst_idx)\n",
    "            label[individual_burst_idx] = label[individual_burst_idx] - ndimage.binary_erosion(\n",
    "                label[individual_burst_idx]\n",
    "            )\n",
    "        label = np.sum(label, axis=0)\n",
    "        label[label > 0] = 1\n",
    "        label = ndimage.binary_dilation(label, structure=struct, iterations=2).astype(label.dtype)\n",
    "        label = np.ma.masked_where(label == 0, label)\n",
    "        class_idx.append(class_idx_curr)\n",
    "        labels.append(label)\n",
    "    return class_idx, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class Visualizer:\n",
    "    \"\"\"A non-entirely abstract class which represents a visualization of a dataset\n",
    "\n",
    "    Args:\n",
    "        data_loader:\n",
    "            A Dataloader to sample from for plotting\n",
    "\n",
    "        visualize_transform:\n",
    "            Defines how to transform the data prior to plotting\n",
    "\n",
    "        visualize_target_transform:\n",
    "            Defines how to transform the target prior to plotting\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        data_loader,\n",
    "        visualize_transform: Optional[Callable] = None,\n",
    "        visualize_target_transform: Optional[Callable] = None,\n",
    "    ) -> None:\n",
    "        self.data_loader = iter(data_loader)\n",
    "        self.visualize_transform = visualize_transform\n",
    "        self.visualize_target_transform = visualize_target_transform\n",
    "\n",
    "    def __iter__(self) -> Iterable:\n",
    "        self.data_iter = iter(self.data_loader)\n",
    "        return self  # type: ignore\n",
    "\n",
    "    def __next__(self) -> Figure:\n",
    "        iq_data, targets = next(self.data_iter)\n",
    "        if self.visualize_transform:\n",
    "            iq_data = self.visualize_transform(iq_data)\n",
    "\n",
    "        if self.visualize_target_transform:\n",
    "            targets = self.visualize_target_transform(targets)\n",
    "\n",
    "        return self._visualize(iq_data, targets)\n",
    "\n",
    "    def _visualize(self, iq_data: np.ndarray, targets: np.ndarray) -> Figure:\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from typing import List, Optional, Callable\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "from copy import deepcopy\n",
    "from matplotlib.figure import Figure\n",
    "\n",
    "\n",
    "class MaskClassVisualizer(Visualizer):\n",
    "    def __init__(\n",
    "        self,\n",
    "        data_loader,\n",
    "        visualize_transform: Optional[Callable] = None,\n",
    "        visualize_target_transform: Optional[Callable] = None,\n",
    "        class_list: Optional[List[str]] = None,\n",
    "        save_dir: Optional[str] = None,\n",
    "    ) -> None:\n",
    "        super(MaskClassVisualizer, self).__init__(data_loader, visualize_transform, visualize_target_transform)\n",
    "        self.class_list = class_list\n",
    "        self.save_dir = save_dir\n",
    "        os.makedirs(self.save_dir, exist_ok=True)\n",
    "        self.images_dir = os.path.join(self.save_dir, \"images\")\n",
    "        os.makedirs(self.images_dir, exist_ok=True)\n",
    "        self.labels_dir = os.path.join(self.save_dir, \"labels\")\n",
    "        os.makedirs(self.labels_dir, exist_ok=True)\n",
    "\n",
    "    def __next__(self) -> Figure:\n",
    "        iq_data, targets = next(self.data_iter)\n",
    "        if self.visualize_transform:\n",
    "            iq_data = self.visualize_transform(deepcopy(iq_data))\n",
    "\n",
    "        if self.visualize_target_transform:\n",
    "            classes, targets = self.visualize_target_transform(deepcopy(targets))\n",
    "        else:\n",
    "            targets = None\n",
    "\n",
    "        return self._visualize(iq_data, targets, classes)\n",
    "\n",
    "    def _visualize(self, data: np.ndarray, targets: np.ndarray, classes: List[str]) -> Figure:\n",
    "        # Create a label encoder and fit it on the class list\n",
    "        label_encoder = LabelEncoder()\n",
    "        label_encoder.fit(self.class_list)\n",
    "\n",
    "        batch_size = data.shape[0]\n",
    "        for sample_idx in range(batch_size):\n",
    "            fig, ax = plt.subplots(figsize=(8, 6))\n",
    "            ax.imshow(\n",
    "                data[sample_idx],\n",
    "                vmin=np.min(data),\n",
    "                vmax=np.max(data),\n",
    "                cmap=\"jet\",\n",
    "                extent=[0, data.shape[2], 0, data.shape[1]],\n",
    "            )\n",
    "            ax.axis('off')\n",
    "\n",
    "            if targets is not None:\n",
    "                class_idx = classes[sample_idx]\n",
    "                mask = targets[sample_idx]\n",
    "                mask_img = np.ma.masked_where(mask < 0.5, mask)\n",
    "                ax.imshow(\n",
    "                    mask_img,\n",
    "                    vmin=np.min(mask),\n",
    "                    vmax=np.max(mask),\n",
    "                    cmap=\"gray\",\n",
    "                    alpha=0.5,\n",
    "                    interpolation=\"none\",\n",
    "                    extent=[0, mask.shape[1], 0, mask.shape[0]],\n",
    "                )\n",
    "\n",
    "            if targets is not None:\n",
    "                class_labels = [self.class_list[idx] for idx in class_idx]\n",
    "                # Encode the class labels as numeric values\n",
    "                encoded_labels = label_encoder.transform(class_labels)\n",
    "                bbox = self._calculate_bounding_box(mask)\n",
    "                title_with_bbox = [f\"{class_label} {bbox_str}\" for class_label, bbox_str in zip(encoded_labels, bbox)]\n",
    "                \n",
    "\n",
    "            # Save the figure as an individual image\n",
    "            filename = f\"spectrogram_{sample_idx}.png\"\n",
    "            filepath = os.path.join(self.images_dir, filename)\n",
    "            plt.savefig(filepath, dpi=300, bbox_inches='tight', pad_inches=0)\n",
    "            plt.close(fig)\n",
    "\n",
    "            # Save labels and bounding boxes in a text file\n",
    "            labels_filepath = os.path.join(self.labels_dir, f\"labels_{sample_idx}.txt\")\n",
    "            with open(labels_filepath, \"w\") as f:\n",
    "                for class_label, bbox in zip(encoded_labels, bbox):\n",
    "                    bbox_str = \" \".join([str(coord) for coord in bbox])\n",
    "                    f.write(f\"{class_label} {bbox_str}\\n\")\n",
    "\n",
    "        return fig\n",
    "\n",
    "    @staticmethod\n",
    "    def _calculate_bounding_box(mask: np.ndarray) -> List[List[float]]:\n",
    "        contours, _ = cv2.findContours(mask.astype(np.uint8), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "        bboxes = []\n",
    "        for contour in contours:\n",
    "            x_coords = contour[:, :, 0].flatten()\n",
    "            y_coords = contour[:, :, 1].flatten()\n",
    "            x_min = np.min(x_coords)\n",
    "            y_min = np.min(y_coords)\n",
    "            x_max = np.max(x_coords)\n",
    "            y_max = np.max(y_coords)\n",
    "\n",
    "            xc = (x_min + x_max) / 2.0\n",
    "            yc = (y_min + y_max) / 2.0\n",
    "            w = x_max - x_min\n",
    "            h = y_max - y_min\n",
    "            \n",
    "            xc = xc/512\n",
    "            yc = yc/512\n",
    "            w = w/512\n",
    "            h = h/512\n",
    "\n",
    "            bbox = [xc, yc, w, h]\n",
    "            bboxes.append(bbox)\n",
    "        return bboxes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    batch_size=120,\n",
    "    shuffle=True,\n",
    ")\n",
    "\n",
    "save_dir = \"train\"\n",
    "\n",
    "visualizer = MaskClassVisualizer(\n",
    "    data_loader=data_loader,\n",
    "    visualize_transform=complex_spectrogram_to_magnitude,\n",
    "    visualize_target_transform=mask_class_to_outline,\n",
    "    class_list=modulation_list,\n",
    "    save_dir=save_dir,\n",
    ")\n",
    "\n",
    "\n",
    "for figure in iter(visualizer):\n",
    "    figure.set_size_inches(16, 9)\n",
    "    plt.show()\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = DataLoader(\n",
    "    dataset=test_dataset,\n",
    "    batch_size=25,\n",
    "    shuffle=True,\n",
    ")\n",
    "\n",
    "save_dir = \"test\"\n",
    "\n",
    "visualizer = MaskClassVisualizer(\n",
    "    data_loader=data_loader,\n",
    "    visualize_transform=complex_spectrogram_to_magnitude,\n",
    "    visualize_target_transform=mask_class_to_outline,\n",
    "    class_list=modulation_list,\n",
    "    save_dir=save_dir,\n",
    ")\n",
    "\n",
    "\n",
    "for figure in iter(visualizer):\n",
    "    figure.set_size_inches(16, 9)\n",
    "    plt.show()\n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9 (pytorch)",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
